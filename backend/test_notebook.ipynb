{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63650dc7",
   "metadata": {},
   "source": [
    "# Test Tools and Models\n",
    "\n",
    "## Running this notebook\n",
    "\n",
    "To run this in VS Code:\n",
    "1. Open a terminal\n",
    "2. Navigate to `./backend`\n",
    "3. Configure poetry to set up a venv environment by running `poetry config virtualenvs.in-project true`\n",
    "4. Remove any existing poetry venv by running `poetry remove --all`\n",
    "5. Run `poetry install` to install the dependencies in the virtual environment\n",
    "6. In Select Kernel dropdown in the top right corner of VS Code, select `Python Environments` and select the venv environment you created at `backend/.venv/bin/python`\n",
    "7. Run the notebook by clicking on the play button in the top right corner of VS Code or running `jupyter notebook test_notebook.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fadff54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:src.tools:Initialized embedding model: all-MiniLM-L6-v2\n",
      "INFO:src.tools:Connected to Milvus at localhost:19530\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:tools:Initialized embedding model: all-MiniLM-L6-v2\n",
      "INFO:tools:Connected to Milvus at localhost:19530\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Get the absolute path to the src directory\n",
    "src_path = os.path.abspath('src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('src/.env')\n",
    "os.environ['MILVUS_HOST'] = 'localhost'\n",
    "os.environ['OLLAMA_HOST'] = 'localhost'\n",
    "from src.models import get_model, bind_tools, handle_tool_call, handle_streaming_tool_call\n",
    "from src.tools import retrieve_context, multiply\n",
    "from src.langgraph_agent import create_agent_graph, run_agent_graph, run_agent_graph_streaming\n",
    "from src.app import ChatRequest, prompt_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbb3cbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:src.langgraph_agent:Running the Agent Graph Streaming Function...\n",
      "INFO:src.langgraph_agent:Running assistant node\n",
      "INFO:src.langgraph_agent:Current messages: ['human']\n",
      "INFO:src.langgraph_agent:Adding system prompt to message history\n",
      "INFO:root:Calling Ollama model with name: qwen2:7b\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x17ac87680>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 22 Apr 2025 16:09:58 GMT'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "INFO:src.langgraph_agent:Assistant response: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308372dfc6954a89a65b2efd048d6bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:src.langgraph_agent:Running final answer async (streaming) node\n",
      "INFO:root:Calling Ollama model with name: qwen2:7b\n",
      "DEBUG:src.langgraph_agent:Debug Stream: <async_generator object final_answer_async.<locals>.wrapped_stream at 0x17bc96b20>\n",
      "DEBUG:src.langgraph_agent:Running streaming wrapper...\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='localhost' port=11434 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x17ac89ac0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/x-ndjson'), (b'Date', b'Tue, 22 Apr 2025 16:10:02 GMT'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST http://localhost:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content=' The' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk:  The\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content=' phone' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk:  phone\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content=' number' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk:  number\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content=' for' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk:  for\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content=' Data' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk:  Data\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content='N' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: N\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content='inja' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: inja\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content=' Support' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk:  Support\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content=' is' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk:  is\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content=' ' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk:  \n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content='1' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: 1\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content='-' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk:  The\n",
      "Chunk:  phone\n",
      "Chunk:  number\n",
      "Chunk:  for\n",
      "Chunk:  Data\n",
      "Chunk: N\n",
      "Chunk: inja\n",
      "Chunk:  Support\n",
      "Chunk:  is\n",
      "Chunk:  \n",
      "Chunk: 1\n",
      "Chunk: -\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:src.langgraph_agent:Streamed chunk: content='8' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: 8\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content='0' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: 0\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content='0' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: 0\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content='-D' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: -D\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content='ATA' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: ATA\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content='-N' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: -N\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content='IN' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: IN\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content='JA' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: JA\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content=',' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: ,\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content=' and' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk:  and\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content=' this' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk:  this\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: 8\n",
      "Chunk: 0\n",
      "Chunk: 0\n",
      "Chunk: -D\n",
      "Chunk: ATA\n",
      "Chunk: -N\n",
      "Chunk: IN\n",
      "Chunk: JA\n",
      "Chunk: ,\n",
      "Chunk:  and\n",
      "Chunk:  this\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:src.langgraph_agent:Streamed chunk: content=' support' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk:  support\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content=' service' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk:  service\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content=' operates' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk:  operates\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content=' ' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk:  \n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content='2' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: 2\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content='4' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: 4\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content='/' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: /\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content='7' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: 7\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content='.' additional_kwargs={} response_metadata={} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49'\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: .\n",
      "DEBUG:src.langgraph_agent:Streamed chunk: content='' additional_kwargs={} response_metadata={'model': 'qwen2:7b', 'created_at': '2025-04-22T16:10:02.915402Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1829742209, 'load_duration': 31545542, 'prompt_eval_count': 1030, 'prompt_eval_duration': 1185773167, 'eval_count': 33, 'eval_duration': 586360541, 'message': Message(role='assistant', content='', images=None, tool_calls=None), 'model_name': 'qwen2:7b'} id='run-67ab1ff0-f5d2-4dd7-b235-cb593f210b49' usage_metadata={'input_tokens': 1030, 'output_tokens': 33, 'total_tokens': 1063}\n",
      "DEBUG:src.langgraph_agent:Debug Chunk: \n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk:  support\n",
      "Chunk:  service\n",
      "Chunk:  operates\n",
      "Chunk:  \n",
      "Chunk: 2\n",
      "Chunk: 4\n",
      "Chunk: /\n",
      "Chunk: 7\n",
      "Chunk: .\n",
      "Chunk: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The phone number for DataNinja Support is 1-800-DATA-NINJA, and this support service operates 24/7.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_notebook.ipynb (Revised)\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "async def test_agent_graph_streaming(user_prompt, agent_graph):\n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=user_prompt)],\n",
    "    }\n",
    "\n",
    "    # Run the agent and collect chunks\n",
    "    collected = []\n",
    "    async for chunk in run_agent_graph_streaming(agent_graph, initial_state):\n",
    "        collected.append(chunk)\n",
    "        print(\"Chunk:\", chunk)  # Print each chunk as it arrives\n",
    "\n",
    "    final_response = \"\".join(collected)\n",
    "    return final_response\n",
    "\n",
    "# Execute the test in Jupyter:\n",
    "assistant_system_prompt = prompt_templates[\"assistant_system_prompt\"]\n",
    "tools = [multiply, retrieve_context]\n",
    "agent_graph = create_agent_graph(tools, assistant_system_prompt, True) # Need to set Agent Graph to True to enable streaming\n",
    "# Test the agent graph with a user prompt\n",
    "user_prompt = \"What is the phone number for DataNinja Support?\"\n",
    "await test_agent_graph_streaming(user_prompt, agent_graph)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1715a2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/LS0tCmNvbmZpZzoKICBmbG93Y2hhcnQ6CiAgICBjdXJ2ZTogbGluZWFyCi0tLQpncmFwaCBURDsKCV9fc3RhcnRfXyhbPHA-X19zdGFydF9fPC9wPl0pOjo6Zmlyc3QKCWFzc2lzdGFudChhc3Npc3RhbnQpCgl0b29scyh0b29scykKCWZpbmFsX2Fuc3dlcihmaW5hbF9hbnN3ZXIpCglfX2VuZF9fKFs8cD5fX2VuZF9fPC9wPl0pOjo6bGFzdAoJX19zdGFydF9fIC0tPiBhc3Npc3RhbnQ7CglmaW5hbF9hbnN3ZXIgLS0-IF9fZW5kX187Cgl0b29scyAtLT4gZmluYWxfYW5zd2VyOwoJYXNzaXN0YW50IC0uLT4gdG9vbHM7Cglhc3Npc3RhbnQgLS4tPiBfX2VuZF9fOwoJY2xhc3NEZWYgZGVmYXVsdCBmaWxsOiNmMmYwZmYsbGluZS1oZWlnaHQ6MS4yCgljbGFzc0RlZiBmaXJzdCBmaWxsLW9wYWNpdHk6MAoJY2xhc3NEZWYgbGFzdCBmaWxsOiNiZmI2ZmMK\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = agent_graph.get_graph().draw_mermaid()\n",
    "# Display the graph in Jupyter\n",
    "\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "def mm(graph):\n",
    "    graphbytes = graph.encode(\"utf8\")\n",
    "    base64_bytes = base64.urlsafe_b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    return Image(url=\"https://mermaid.ink/img/\" + base64_string)\n",
    "\n",
    "img = mm(graph)\n",
    "\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9b016",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "run_agent_graph() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m initial_state = {\n\u001b[32m      2\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(content=user_prompt)],\n\u001b[32m      3\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstreaming\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Ensure streaming is enabled in state\u001b[39;00m\n\u001b[32m      4\u001b[39m     }\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response = \u001b[43mrun_agent_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[31mTypeError\u001b[39m: run_agent_graph() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "initial_state = {\n",
    "        \"messages\": [HumanMessage(content=user_prompt)],\n",
    "    }\n",
    "\n",
    "agent_graph = create_agent_graph(tools, assistant_system_prompt, False)\n",
    "# Test the agent graph with a user prompt\n",
    "response = run_agent_graph(agent_graph, initial_state)\n",
    "print('\\n\\n\\n')\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
